{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Referenced from https://github.com/ikostrikov/TensorFlow-Pointer-Networks/blob/master/dataset.py\n",
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Construct a DataGenerator.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def next_batch(self, batch_size, N, train_mode=True):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\n",
    "        # A sequence of random numbers from [0, 1]\n",
    "        reader_input_batch = []\n",
    "\n",
    "        # Sorted sequence that we feed to encoder\n",
    "        # In inference we feed an unordered sequence again\n",
    "        decoder_input_batch = []\n",
    "\n",
    "        # Ordered sequence where one hot vector encodes position in the input array\n",
    "        writer_outputs_batch = []\n",
    "        for _ in range(N):\n",
    "            reader_input_batch.append(np.zeros([batch_size, 1]))\n",
    "        for _ in range(N + 1):\n",
    "            decoder_input_batch.append(np.zeros([batch_size, 1]))\n",
    "            writer_outputs_batch.append(np.zeros([batch_size, N + 1]))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            shuffle = np.random.permutation(N)\n",
    "            sequence = np.sort(np.random.randint(0, 100, N))\n",
    "            shuffled_sequence = sequence[shuffle]\n",
    "\n",
    "            for i in range(N):\n",
    "                reader_input_batch[i][b] = shuffled_sequence[i]\n",
    "                if train_mode:\n",
    "                    decoder_input_batch[i + 1][b] = sequence[i]\n",
    "                else:\n",
    "                    decoder_input_batch[i + 1][b] = shuffled_sequence[i]\n",
    "                writer_outputs_batch[shuffle[i]][b, i + 1] = 1.0\n",
    "\n",
    "            # Points to the stop symbol\n",
    "            writer_outputs_batch[N][b, 0] = 1.0\n",
    "\n",
    "        return reader_input_batch, decoder_input_batch, writer_outputs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_cell = tf.contrib.rnn.LSTMCell\n",
    "static_rnn = tf.nn.static_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "batch_size = 100\n",
    "input_max_len = 10\n",
    "learning_rate = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "uniform_initializer = tf.random_uniform_initializer(-.1, .1)\n",
    "normal_initializer = tf.truncated_normal_initializer(.0, .1)\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32, [batch_size, 1], \"enc_input_{}\".format(i)) for i in range(input_max_len)]\n",
    "targets = [tf.placeholder(tf.float32, [batch_size, input_max_len+1], \"dec_target_{}\".format(i)) for i in range(input_max_len+1)]\n",
    "target_weights = [tf.placeholder(tf.float32, [batch_size, 1], \"target_weights_{}\".format(i)) for i in range(input_max_len+1)]\n",
    "\n",
    "# Encoding\n",
    "enc_cell = lstm_cell(lstm_size, initializer=uniform_initializer)\n",
    "enc_outputs, enc_state = static_rnn(enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "end_of_sentence = tf.zeros([batch_size, lstm_size])\n",
    "enc_outputs = [end_of_sentence] + enc_outputs\n",
    "\n",
    "dec_state = enc_state\n",
    "\n",
    "# enc_output.shape : batch_size * input_max_len * lstm_size\n",
    "dec_inputs = [tf.placeholder(tf.float32, [batch_size, 1], \"dec_inputs_{}\".format(i)) for i in range(input_max_len+1)]\n",
    "dec_cell = lstm_cell(lstm_size, initializer=uniform_initializer)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", [input_max_len, lstm_size], initializer=normal_initializer)\n",
    "W2 = tf.get_variable(\"W2\", [input_max_len, lstm_size], initializer=normal_initializer)\n",
    "v = tf.get_variable(\"v\", [input_max_len, 1], initializer=normal_initializer)\n",
    "\n",
    "with tf.variable_scope(\"decode\") as scope:\n",
    "    predictions = []\n",
    "    for i, dec_input in enumerate(dec_inputs):\n",
    "        if i > 0:\n",
    "            scope.reuse_variables()\n",
    "        \n",
    "        w_d = tf.get_variable(\"w_d\", [1, lstm_size], initializer=normal_initializer)\n",
    "        b_d = tf.get_variable(\"b_d\", [lstm_size], initializer=tf.zeros_initializer())\n",
    "\n",
    "        cell_input = tf.nn.elu(tf.matmul(dec_input, w_d) + b_d)\n",
    "        dec_output, dec_state = dec_cell(cell_input, dec_state)\n",
    "        \n",
    "        u_i = []\n",
    "        for j, enc_output in enumerate(enc_outputs):\n",
    "            W1ej = tf.matmul(enc_output, tf.transpose(W1))\n",
    "            W2di = tf.matmul(dec_output, tf.transpose(W2))\n",
    "            u_i_j = tf.matmul(tf.tanh(W1ej +W2di), v)\n",
    "            u_i.append(u_i_j)\n",
    "        u_i = tf.concat(u_i, axis=1)\n",
    "        p_c_i = tf.nn.softmax(u_i)\n",
    "        predictions.append(p_c_i)\n",
    "\n",
    "# For prediction\n",
    "predictions_idx = tf.argmax(predictions, axis=2)\n",
    "        \n",
    "loss = tf.sqrt(tf.reduce_mean(tf.pow(tf.stack(predictions) - tf.stack(targets), 2.0)))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.28756311535835266 / Validation: 0.28734520077705383\n",
      "\tInput     : [10 37 56 22 71 23 89 85 63  9]\n",
      "\tPrediction: [9 9 9 9 9 9 9 9 9 9]\n",
      "Loss: 0.16160115599632263 / Validation: 0.1540466994047165\n",
      "\tInput     : [47  5 90  3 61 83  7 87 55 10]\n",
      "\tPrediction: [ 3  5  7 10 47 61 61 83 90 90]\n",
      "Loss: 0.14069871604442596 / Validation: 0.12411071360111237\n",
      "\tInput     : [79 43 78 60 46 74 10 70  6 10]\n",
      "\tPrediction: [ 6 10 10 43 46 60 70 74 78 78]\n",
      "Loss: 0.12289181351661682 / Validation: 0.11063995212316513\n",
      "\tInput     : [47 26 28 40 40 61 23 39 78 72]\n",
      "\tPrediction: [23 26 28 39 39 40 47 61 72 78]\n",
      "Loss: 0.11292976140975952 / Validation: 0.12545043230056763\n",
      "\tInput     : [44 47 77 18 57 61 79 29 37 87]\n",
      "\tPrediction: [18 29 37 44 44 57 61 77 79 87]\n",
      "Loss: 0.10825784504413605 / Validation: 0.10536710172891617\n",
      "\tInput     : [60 62 62 24 13 10 82 18 36  6]\n",
      "\tPrediction: [ 6 10 13 18 24 36 60 62 62 82]\n",
      "Loss: 0.10708148777484894 / Validation: 0.11803729832172394\n",
      "\tInput     : [15  6 72 13  9 37 76 71 67 31]\n",
      "\tPrediction: [ 6  9 13 15 31 37 67 71 72 76]\n",
      "Loss: 0.10396937280893326 / Validation: 0.11096074432134628\n",
      "\tInput     : [35 81 24 80 55 33 12 43 16  3]\n",
      "\tPrediction: [ 3 12 16 24 33 35 43 55 80 81]\n",
      "Loss: 0.10325896739959717 / Validation: 0.12038394808769226\n",
      "\tInput     : [95 97 61 59 53  2 17 65 15  5]\n",
      "\tPrediction: [ 2  5 15 17 53 59 61 65 95 97]\n",
      "Loss: 0.11158176511526108 / Validation: 0.10213268548250198\n",
      "\tInput     : [46 14 51 15 21 96 86 78 34 28]\n",
      "\tPrediction: [14 15 21 28 34 46 51 78 86 96]\n",
      "Loss: 0.10058789700269699 / Validation: 0.09953320771455765\n",
      "\tInput     : [84 80 12 70 41 52 92  1 24 56]\n",
      "\tPrediction: [ 1 12 24 41 52 56 70 80 84 92]\n",
      "Loss: 0.09578152745962143 / Validation: 0.09840136766433716\n",
      "\tInput     : [ 9 57 86 68 55  1 15 68 92 58]\n",
      "\tPrediction: [ 1  9 15 55 58 58 68 68 86 92]\n",
      "Loss: 0.10341662168502808 / Validation: 0.10468032211065292\n",
      "\tInput     : [ 3 46  5 43 84 84  7 22 57  0]\n",
      "\tPrediction: [ 0  3  5  7 22 43 46 57 84 84]\n",
      "Loss: 0.10787001997232437 / Validation: 0.09716969728469849\n",
      "\tInput     : [33  4 49 25 81 22 35 96 99 73]\n",
      "\tPrediction: [ 4 22 25 33 35 49 73 81 96 99]\n",
      "Loss: 0.08795522153377533 / Validation: 0.09400559216737747\n",
      "\tInput     : [63 77 28 95 43 45 81 15 58 27]\n",
      "\tPrediction: [15 27 28 43 45 58 63 77 81 95]\n",
      "Loss: 0.09194286167621613 / Validation: 0.10244682431221008\n",
      "\tInput     : [49 67 72 18 11 96 88 43 88  2]\n",
      "\tPrediction: [ 2 11 18 43 49 67 72 88 88 96]\n",
      "Loss: 0.09911656379699707 / Validation: 0.09948457777500153\n",
      "\tInput     : [14  7 93 86 56 34 76 16 67 30]\n",
      "\tPrediction: [ 7 14 16 30 34 56 67 76 86 93]\n",
      "Loss: 0.10402177274227142 / Validation: 0.11468973010778427\n",
      "\tInput     : [32 56 86 75 44  8 65 29 24 63]\n",
      "\tPrediction: [ 8 24 29 32 44 56 63 65 75 86]\n",
      "Loss: 0.09461680054664612 / Validation: 0.10110962390899658\n",
      "\tInput     : [77 21 31 51 81 87 65  3 48 71]\n",
      "\tPrediction: [ 3 21 31 48 51 65 71 77 81 87]\n",
      "Loss: 0.10603974759578705 / Validation: 0.09896860271692276\n",
      "\tInput     : [71 93 73 43 78 25 97  1 31 21]\n",
      "\tPrediction: [ 1 21 25 31 43 71 73 78 93 97]\n",
      "Loss: 0.10393859446048737 / Validation: 0.09298989176750183\n",
      "\tInput     : [59 54 27 40 97 57 26 53  3 55]\n",
      "\tPrediction: [ 3 26 27 40 53 53 55 57 59 97]\n",
      "Loss: 0.09895256906747818 / Validation: 0.09821245074272156\n",
      "\tInput     : [46 27 53  8 11 14 50  3 52 27]\n",
      "\tPrediction: [ 3  8 11 14 27 27 46 50 53 53]\n",
      "Loss: 0.10629430413246155 / Validation: 0.10128885507583618\n",
      "\tInput     : [21 79 64 43 32  8 25 95 47 21]\n",
      "\tPrediction: [ 8 21 21 25 32 43 47 64 79 95]\n",
      "Loss: 0.10616669058799744 / Validation: 0.10104629397392273\n",
      "\tInput     : [92 37  0 32 48 25 17 27 86 64]\n",
      "\tPrediction: [ 0 17 25 27 32 37 48 64 86 92]\n",
      "Loss: 0.09967442601919174 / Validation: 0.09911146759986877\n",
      "\tInput     : [35 38 57 14 37 28 37 76 89 35]\n",
      "\tPrediction: [14 28 28 35 38 38 38 57 76 89]\n",
      "Loss: 0.09939052909612656 / Validation: 0.089706189930439\n",
      "\tInput     : [40 71 63 23 96 43 53 43 85 48]\n",
      "\tPrediction: [23 40 43 48 48 53 63 71 85 96]\n",
      "Loss: 0.08265111595392227 / Validation: 0.09522303938865662\n",
      "\tInput     : [77 32 66 53 73 71 12 87 63 85]\n",
      "\tPrediction: [12 32 53 63 66 71 73 77 85 87]\n",
      "Loss: 0.09245359897613525 / Validation: 0.09179584681987762\n",
      "\tInput     : [83 68 84 81 67  6 42  6 28 73]\n",
      "\tPrediction: [ 6  6 28 42 67 68 73 81 83 84]\n",
      "Loss: 0.09832067787647247 / Validation: 0.10091521590948105\n",
      "\tInput     : [15 89 14 54 95 54  3 17 58 73]\n",
      "\tPrediction: [ 3 14 17 17 54 58 58 73 89 95]\n",
      "Loss: 0.10327627509832382 / Validation: 0.10371575504541397\n",
      "\tInput     : [49 72 19 40 15 69 94 37 43 71]\n",
      "\tPrediction: [15 19 37 40 43 49 69 71 72 94]\n",
      "Loss: 0.09384205937385559 / Validation: 0.08868709951639175\n",
      "\tInput     : [77  2  9 57 18 78 29 95 31 97]\n",
      "\tPrediction: [ 2  9 18 29 31 57 77 78 95 97]\n",
      "Loss: 0.09992891550064087 / Validation: 0.10480212420225143\n",
      "\tInput     : [99 25 23 83 56 26 90 63 50 94]\n",
      "\tPrediction: [23 25 26 50 56 63 83 90 94 99]\n",
      "Loss: 0.09188467264175415 / Validation: 0.09385253489017487\n",
      "\tInput     : [61 23 77 92  4  7 59 81 28 41]\n",
      "\tPrediction: [ 4  7 23 28 41 59 61 77 81 92]\n",
      "Loss: 0.10511653125286102 / Validation: 0.1068129613995552\n",
      "\tInput     : [66 22 49 11 52  7 73 52 11 57]\n",
      "\tPrediction: [ 7 11 11 22 49 52 52 57 66 73]\n",
      "Loss: 0.10227023810148239 / Validation: 0.10647152364253998\n",
      "\tInput     : [ 2 11  5 45 45 90 99 71 21 39]\n",
      "\tPrediction: [ 2  5 11 21 39 45 45 71 90 99]\n",
      "Loss: 0.08154284954071045 / Validation: 0.09195093810558319\n",
      "\tInput     : [56 82 69 59 45 30 17 29 42 22]\n",
      "\tPrediction: [17 22 29 30 42 45 56 59 69 82]\n",
      "Loss: 0.09232892096042633 / Validation: 0.09690837562084198\n",
      "\tInput     : [33 59 67 14 19 68 67 52 44 60]\n",
      "\tPrediction: [14 19 33 44 52 59 60 67 68 68]\n",
      "Loss: 0.09618252515792847 / Validation: 0.09678885340690613\n",
      "\tInput     : [26 53  8  8 40 26 18 39  5 95]\n",
      "\tPrediction: [ 5  8  8 18 26 26 39 40 53 95]\n",
      "Loss: 0.10221855342388153 / Validation: 0.10207311809062958\n",
      "\tInput     : [86 89 22 27 25 84 34 75 57 57]\n",
      "\tPrediction: [22 25 27 34 57 57 75 84 86 89]\n",
      "Loss: 0.09460984915494919 / Validation: 0.09939048439264297\n",
      "\tInput     : [45 62 83  5 72  8 40  6 36 19]\n",
      "\tPrediction: [ 5  6  8 19 36 40 45 62 72 83]\n",
      "Loss: 0.10924463719129562 / Validation: 0.10564873367547989\n",
      "\tInput     : [59 79 35 87 40 19 56 19 61  3]\n",
      "\tPrediction: [ 3 19 19 35 40 56 59 61 79 87]\n",
      "Loss: 0.09893127530813217 / Validation: 0.1047142818570137\n",
      "\tInput     : [30 96 45 93 93  1 43 27 78 76]\n",
      "\tPrediction: [ 1 27 30 43 45 76 78 93 96 96]\n",
      "Loss: 0.08232612907886505 / Validation: 0.09626010060310364\n",
      "\tInput     : [30 79 28 15 95 86 38 53 84 94]\n",
      "\tPrediction: [15 28 30 38 53 79 84 86 94 95]\n",
      "Loss: 0.08562035113573074 / Validation: 0.09230824559926987\n",
      "\tInput     : [81 23 99 66 22 90 43 96 60 92]\n",
      "\tPrediction: [22 23 43 60 66 81 90 92 99 99]\n",
      "Loss: 0.09962673485279083 / Validation: 0.10277918726205826\n",
      "\tInput     : [63 38 92 28 79 30 79  6 51 38]\n",
      "\tPrediction: [ 6 28 30 38 38 51 63 79 79 92]\n",
      "Loss: 0.11485303193330765 / Validation: 0.11118301004171371\n",
      "\tInput     : [64 14 79 24  4 36 52  1 90 77]\n",
      "\tPrediction: [ 1  4 14 24 36 52 64 77 79 90]\n",
      "Loss: 0.09158843010663986 / Validation: 0.09464704245328903\n",
      "\tInput     : [48 86 10  7 65 45 81 24 85 33]\n",
      "\tPrediction: [ 7 10 24 33 45 48 65 81 85 86]\n",
      "Loss: 0.09611586481332779 / Validation: 0.10265220701694489\n",
      "\tInput     : [84 31 84 15 28 31 20 19 14 92]\n",
      "\tPrediction: [14 15 19 20 28 31 31 84 84 92]\n",
      "Loss: 0.09345538169145584 / Validation: 0.08793973922729492\n",
      "\tInput     : [10 37 48 71 62 46 23 55 37 32]\n",
      "\tPrediction: [10 23 32 37 37 46 48 55 62 71]\n",
      "Loss: 0.09813698381185532 / Validation: 0.10552307963371277\n",
      "\tInput     : [ 5 34 25 54 32 22 49 48 79 87]\n",
      "\tPrediction: [ 5 22 25 32 34 48 49 54 79 87]\n"
     ]
    }
   ],
   "source": [
    "init= tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    datagen = DataGenerator()\n",
    "    \n",
    "    def create_feed_dict(input_data, dec_input_data, dec_target_data):\n",
    "        feed_dict = {}\n",
    "        for placeholder, data in zip(inputs, input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "        for placeholder, data in zip(dec_inputs, dec_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "        for placeholder, data in zip(targets, dec_target_data):\n",
    "            feed_dict[placeholder] = data\n",
    "        for placeholder in target_weights:\n",
    "            feed_dict[placeholder] = np.ones([batch_size, 1])\n",
    "        \n",
    "        return feed_dict\n",
    "    \n",
    "    for i in range(10000):\n",
    "        input_data, dec_input_data, dec_target_data = datagen.next_batch(batch_size, input_max_len)\n",
    "        train_feed_dict = create_feed_dict(input_data, dec_input_data, dec_target_data)\n",
    "        \n",
    "        loss_val, _ = sess.run([loss, train_op], feed_dict=train_feed_dict)\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            test_input_data, test_dec_input_data, test_dec_target_data = datagen.next_batch(batch_size, input_max_len)\n",
    "            test_feed_dict = create_feed_dict(test_input_data, test_dec_input_data, test_dec_target_data)\n",
    "            test_loss_val, idx = sess.run([loss, predictions_idx], feed_dict=test_feed_dict)\n",
    "            \n",
    "            print(\"Loss: {} / Validation: {}\".format(loss_val, test_loss_val))\n",
    "            \n",
    "            sample_test_input_data = np.transpose(test_input_data, (1, 0, 2))[0].flatten()\n",
    "            sample_idx = np.transpose(idx)[0][:-1] - 1\n",
    "            predict_arr = [sample_test_input_data[i] for i in sample_idx]\n",
    "            print(\"\\t{:10}: {}\".format(\"Input\", np.array(sample_test_input_data).astype(int)))\n",
    "            print(\"\\t{:10}: {}\".format(\"Prediction\", np.array(predict_arr).astype(int)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
